# aiscripts

experiments integrating with openAI and langchain using node scripts (and own keys)

Both openAI and langchain provide means to integrate with node javascript which this repo uses

## setup

- node 20 required and `yarn install`
- expects your openAI key in an env variable [uses node20 env](https://nodejs.org/en/learn/command-line/how-to-read-environment-variables-from-nodejs)

## scripts

- index.js integration to openAI using langchain,, follows [this langchain tutorial](https://js.langchain.com/docs/tutorials/llm_chain/)

- openAIChat.js an integration to openAI directly,

## run scripts

- index.js run it: `node --env-file=.env index.js`
- openAIChat.js run it: `node --env-file=.env openAIChat.js`

## langchain

langchain is an open source software (oss) framework to build ai apps with python and javascript

- integrates with over 15 LLMs ("future proof your app by incorporating vendor optionality")
- provides tool usage to integrate with your own apis, vector dbs, sql, document loaders, search engines and more (over 600 integrations total)

langchain prompt templates look like a useful facade to simplify complex messages

langchain supports chaining multiple steps together

### langgraph

langchains product to build stateful agents (not oss); integrates with langchain but can be used independently

### langchain chat models

langchain chat models are an abstraction layer over llms; pass a list of messages and get back a message in response; langchain chat models provide:

- a consistent interface for working with multiple LLMs
- standard tool calling api
- formats, caching, token usage and more configuration options (timeout, tempoerature etc.)
- python and typescript

## chat terminology

(quotes from the openAI docs)

"User" messages contain instructions that request a particular type of output from the model. You can think of user messages as the messages you might type in to ChatGPT as an end user.

Messages with the "developer" role provide instructions to the model that are prioritized ahead of user messages. They typically describe how the model should generally behave and respond. This message role used to be called the system prompt which it replaces.

Messages with the "assistant" role are presumed to have been generated by the model, perhaps in a previous generation request (see the "Conversations" section below). They can also be used to provide examples to the model for how it should respond to the current request - a technique known as few-shot learning.

Message types can also be used to provide additional information to the model which may be outside its training data. You might want to include the results of a database query, a text document, or other resources to help the model generate a relevant response. This technique is often referred to as retrieval augmented generation, or RAG.

Each text generation request is independent and stateless (unless you are using assistants), you can still implement multi-turn conversations by providing additional messages as parameters to your text generation request. By using alternating user and assistant messages, you can capture the previous state of a conversation in one request to the model.

"few-shot prompting": the technique of adding example inputs and expected outputs to a model prompt is known as "few-shot prompting"

- The simplest types of examples just have a user input and an expected model output. These are single-turn examples.
- One more complex type if example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer. This is called a multi-turn example.

[Retrieval Augmented Generation (RAG)](https://python.langchain.com/docs/concepts/rag/) is a powerful technique that enhances language models by combining them with external knowledge bases. RAG addresses a key limitation of models: models rely on fixed training datasets, which can lead to outdated or incomplete information. When given a query, RAG systems first search a knowledge base for relevant information. The system then incorporates this retrieved information into the model's prompt.

tool calling (aka function calling) allows model to interact directly with systems which probably have a specific schema. with tool calling you can describe your tools to the model so it knows how to call. [openAI calls it function calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) and is supported since gpt-4-turbo. The openAI example shows an function integration which given an orderId will return the order date

### Tokens

- Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens.
  - Guide: 1 token ~= 4 chars in English.
  - Guide: 100 tokens ~= 75 words
- Depending on the model used, requests can use up to 128,000 tokens shared between prompt and completion.
- Output tokens are the tokens that are generated by a model in response to a prompt. Each model supports different limits for output tokens
- A context window describes the total tokens that can be used for both input tokens and output tokens

example prompts: https://platform.openai.com/docs/examples
e.g. "You will be provided with a piece of Python code, and your task is to provide ideas for efficiency improvements."

## improve efficiency

Improvements to accuracy, cost and latency [openAI docs](https://platform.openai.com/docs/guides/optimizing-llm-accuracy)

"The typical LLM task will start with prompt engineering, where we test, learn, and evaluate to get a baseline. Once weâ€™ve reviewed those baseline examples and assessed why they are incorrect, we can do more including

- Context optimization: e.g. RAG
- LLM optimization: e.g. tone or style of speech, not follow reasoning etc."
