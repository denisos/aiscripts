#

### terms related to chat

"User" messages contain instructions that request a particular type of output from the model. You can think of user messages as the messages you might type in to ChatGPT as an end user.

Messages with the "developer" role provide instructions to the model that are prioritized ahead of user messages. They typically describe how the model should generally behave and respond. This message role used to be called the system prompt which it replaces.

Messages with the "assistant" role are presumed to have been generated by the model, perhaps in a previous generation request (see the "Conversations" section below). They can also be used to provide examples to the model for how it should respond to the current request - a technique known as few-shot learning.

Message types can also be used to provide additional information to the model which may be outside its training data. You might want to include the results of a database query, a text document, or other resources to help the model generate a relevant response. This technique is often referred to as retrieval augmented generation, or RAG.

Each text generation request is independent and stateless (unless you are using assistants), you can still implement multi-turn conversations by providing additional messages as parameters to your text generation request. By using alternating user and assistant messages, you can capture the previous state of a conversation in one request to the model.

Tokens.

- Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens.
  ** Guide: 1 token ~= 4 chars in English.
  ** Guide: 100 tokens ~= 75 words
- Depending on the model used, requests can use up to 128,000 tokens shared between prompt and completion.
- Output tokens are the tokens that are generated by a model in response to a prompt. Each model supports different limits for output tokens
- A context window describes the total tokens that can be used for both input tokens and output tokens

example prompts: https://platform.openai.com/docs/examples
e.g. "You will be provided with a piece of Python code, and your task is to provide ideas for efficiency improvements."

### improve efficiency

Improvements to accuracy, cost and latency

The typical LLM task will start with prompt engineering, where we test, learn, and evaluate to get a baseline. Once weâ€™ve reviewed those baseline examples and assessed why they are incorrect, we can do more including

- Context optimization: e.g. RAG
- LLM optimization: e.g. tone or style of speech, not follow reasoning

details: https://platform.openai.com/docs/guides/optimizing-llm-accuracy
